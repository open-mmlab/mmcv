// Copyright (c) 2021, SenseTime.

#include <float.h>

#define NFU_ALIGN_SIZE 128          // Byte
#define REM_FOR_STACK (128 * 1024)  // 128KB reserved for cncc

#ifdef __BANG_ARCH__
#define MAX_NRAM_SIZE \
  (__MLU_NRAM_SIZE__ * 1024 - REM_FOR_STACK)  // 128KB reserved for cncc
#else
#define MAX_NRAM_SIZE (384 * 1024)  // 384KB, initialization value
#endif

#ifndef PAD_UP
#define PAD_UP(x, y) (((x) / (y) + (int)((x) % (y) > 0)) * (y))
#endif

__nram__ char nram_buff[MAX_NRAM_SIZE];

template <typename T>
__mlu_func__ void loadInput(char* nram_input, T* dram_input, int32_t size,
                            int32_t dst_stride = 0, int32_t src_stride = 0,
                            int32_t count = 1) {
  __memcpy_async(nram_input, dram_input, size, GDRAM2NRAM, dst_stride,
                 src_stride, count - 1);
}

template <typename T>
__mlu_func__ void storeOutput(T* dram_output, char* nram_output, int32_t size,
                              int32_t dst_stride = 0, int32_t src_stride = 0,
                              int32_t count = 1) {
  __memcpy_async(dram_output, nram_output, size, NRAM2GDRAM, dst_stride,
                 src_stride, count - 1);
}

template <typename T>
__mlu_func__ void compute(T* input, const int32_t* target, const T* weight,
                          const float has_weight, const int32_t deal_num,
                          const int32_t n_seg, const int32_t C, float alpha,
                          float gamma, T* scalar_temp, T* tensor_max,
                          T* tensor_temp, T* output) {
  const int32_t scalar_elem_num = NFU_ALIGN_SIZE / sizeof(T);

  // 0. n_max = max(0, x)
  __nramset((T*)tensor_max, deal_num, (T)0);
  __bang_cycle_maxequal((T*)tensor_max, (T*)tensor_max, (T*)input, deal_num,
                        deal_num);

  // 1. ln(1+e^x) = ln(e^(-max) + e^(x-max)) + max
  __nramset((T*)scalar_temp, scalar_elem_num, (T)-1);
  __bang_cycle_mul((T*)tensor_temp, (T*)tensor_max, (T*)scalar_temp, deal_num,
                   scalar_elem_num);
  __bang_cycle_add((T*)output, (T*)input, (T*)tensor_temp, deal_num, deal_num);
  __bang_active_exphp((T*)output, (T*)output, deal_num);
  __bang_active_exphp((T*)tensor_temp, (T*)tensor_temp, deal_num);
  __bang_cycle_add((T*)output, (T*)output, (T*)tensor_temp, deal_num, deal_num);
  __bang_active_loghp((T*)output, (T*)output, deal_num);
  __bang_cycle_add((T*)output, (T*)output, (T*)tensor_max, deal_num, deal_num);

  // 2. temp = [1 + e^(-x)] ^ (-r)
  __nramset((T*)scalar_temp, scalar_elem_num, (T)-1);
  __bang_cycle_mul((T*)tensor_temp, (T*)input, (T*)scalar_temp, deal_num,
                   scalar_elem_num);
  __bang_active_exphp((T*)tensor_temp, (T*)tensor_temp, deal_num);

  __nramset((T*)scalar_temp, scalar_elem_num, (T)1);
  __bang_cycle_add((T*)tensor_temp, (T*)tensor_temp, (T*)scalar_temp, deal_num,
                   scalar_elem_num);
  __bang_active_loghp((T*)tensor_temp, (T*)tensor_temp, deal_num);

  __nramset((T*)scalar_temp, scalar_elem_num, (T)(-gamma));
  __bang_cycle_mul((T*)tensor_temp, (T*)tensor_temp, (T*)scalar_temp, deal_num,
                   scalar_elem_num);
  __bang_active_exphp((T*)tensor_temp, (T*)tensor_temp, deal_num);

  // 3.1 output: target != j
  __nramset((T*)scalar_temp, scalar_elem_num, (T)(1 - alpha));
  __bang_cycle_mul((T*)output, (T*)output, (T*)scalar_temp, deal_num,
                   scalar_elem_num);
  __bang_cycle_mul((T*)output, (T*)output, (T*)tensor_temp, deal_num, deal_num);

  // 3.2 output: target == j
  const int32_t c_align_size = PAD_UP((sizeof(T) * C), NFU_ALIGN_SIZE);
  for (int32_t i = 0; i < n_seg; ++i) {
    const int32_t target_value = *((int32_t*)target + i);
    if (target_value >= 0 && target_value < C) {
      const int32_t offset = i * c_align_size + target_value * sizeof(T);
      char* addr_input = (char*)input + offset;
      char* addr_output = (char*)output + offset;
      const float x = *(T*)addr_input;
      const float p = 1. / (1. + exp(-x));
      *(T*)addr_output = -alpha * pow(1. - p, gamma) * log(fmax(p, FLT_MIN));
    }
  }

  // with weight
  if (has_weight > 0) {
    int32_t row_num_elem = deal_num / n_seg;
    for (int32_t i = 0; i < n_seg; ++i) {
      const int32_t t = *((int32_t*)target + i);
      __nramset((T*)scalar_temp, scalar_elem_num, *((T*)weight + t));
      __bang_cycle_mul((T*)output + i * row_num_elem,
                       (T*)output + i * row_num_elem, (T*)scalar_temp,
                       row_num_elem, scalar_elem_num);
    }
  }
}

template <typename T>
__mlu_func__ void focalLossSigmoidForwardBlock(
    const T* input, const int32_t* target, const T* weight,
    const int32_t row_num, const int32_t C, const float alpha,
    const float gamma, T* output) {
  /*
   * NRAM partition
   *  |-----------------------------------------------------------------------|
   *  |                                scalar |
   *  |-----------------------------------------------------------------------|
   *  |                                weight |
   *  |------------------------------- COMPUTE
   * -------------------------------| |                                   | |
   *  |              computeA             |               computeB | | | |
   *  |------------- PING ------------------------------- PONG
   * ---------------| |                                   | | | input | input
   * | |                                   | |
   *  |-----------------------------------|-----------------------------------|
   *  |                                   | | |              output | output |
   *  |                                   | |
   *  |-----------------------------------|-----------------------------------|
   *  |              target               |               target |
   *  |-----------------------------------|-----------------------------------|
   *
   * split_pipeline_num is 6: COMPUTE(computeA,computeB), PING(input,output),
   * PONG(input,output). split_target_num is 2: PING(target), PONG(target).
   */
  const int32_t c_align = PAD_UP(C, NFU_ALIGN_SIZE / sizeof(T));
  const int32_t c_align_size = c_align * sizeof(T);
  const int32_t scalar_size = NFU_ALIGN_SIZE;
  const int32_t weight_size = (weight != NULL) * c_align_size;
  const int32_t split_pipeline_num = 6;
  const int32_t split_target_num = 2;

  const int32_t remain_size = MAX_NRAM_SIZE - scalar_size - weight_size;
  const int32_t n_seg = remain_size / (split_pipeline_num * c_align_size +
                                       split_target_num * sizeof(int32_t));
  const int32_t deal_num = n_seg * c_align_size / sizeof(T);
  const int32_t target_size = n_seg * sizeof(int32_t);

  // nram scalar,weight
  char* nram_scalar = (char*)nram_buff;
  char* nram_weight = (char*)nram_scalar + scalar_size;
  if (weight_size > 0) {
    loadInput<T>(nram_weight, (T*)weight, C * sizeof(T));
    __asm__ volatile("sync;");
  }

  // nram COMPUTE
  const int32_t compute_size = 2 * c_align_size * n_seg;
  char* nram_compute_a = (char*)nram_weight + weight_size;
  char* nram_compute_b = (char*)nram_compute_a + c_align_size * n_seg;

  // nram PING/PONG
  const int32_t pingpong_offset = (remain_size - compute_size) / 2;
  char* nram_input = (char*)nram_compute_a + 2 * c_align_size * n_seg;
  char* nram_output = (char*)nram_compute_a + 3 * c_align_size * n_seg;
  char* nram_target = (char*)nram_compute_a + 4 * c_align_size * n_seg;

  const int32_t repeat = row_num / n_seg;
  const int32_t remain = row_num % n_seg;

  /*
   * Pipeline: The pipeline is processed in three stages: Load, Compute,
   * Store. The allocated memory space of NRAM is divided into two parts: PING
   *           and Pong. In a single time slice, PING is used to process IO
   * stream and PONG is used for computation. Both of them are processed
   *           synchronously until finished.
   *
   * diagram of PINGPONG:
   * |--------|-----------------------------------------------------------------|
   * |        |                              space |
   * |--------|-----------------------------------------------------------------|
   * |        |   Ping   |   Pong   |   Ping   |   Pong   |   Ping   |   Pong
   * |
   * |--------|-----------------------------------------------------------------|
   * | time 0 |    L0    |          |          |          |          | | | 1 |
   * C0    |    L1    |          |          |          |          | |      2 |
   * S0    |    C1    |    L2    |          |          |          | |      3 |
   * |    S1    |    C2    |    L3    |          |          | |      4 | | |
   * S2    |    C3    |    L4    |          | |      5 |          |          |
   * |    S3    |    C4    |    L5    | |      6 |          |          | | |
   * S4    |    C5    | |      7 |          |          |          |          |
   * |    S5    |
   * |--------|-----------------------------------------------------------------|
   */

  // diagram of PINGPONG: L0
  if (repeat > 0) {
    loadInput<T>(nram_input, (T*)input, C * sizeof(T), c_align * sizeof(T),
                 C * sizeof(T), n_seg);
    loadInput<int32_t>(nram_target, (int32_t*)target, target_size);
    __asm__ volatile("sync;");
  }

  // diagram of PINGPONG: C0 and L1
  if (repeat > 1) {
    loadInput<T>(nram_input + pingpong_offset, (T*)input + C * n_seg,
                 C * sizeof(T), c_align * sizeof(T), C * sizeof(T), n_seg);
    loadInput<int32_t>(nram_target + pingpong_offset, (int32_t*)target + n_seg,
                       target_size);
    compute((T*)nram_input, (int32_t*)nram_target, (T*)nram_weight, weight_size,
            deal_num, n_seg, C, alpha, gamma, (T*)nram_scalar,
            (T*)nram_compute_a, (T*)nram_compute_b, (T*)nram_output);
    __asm__ volatile("sync;");
  }

  for (int32_t i = 0; i < repeat - 2; ++i) {
    storeOutput<T>((T*)output + i * C * n_seg,
                   nram_output + (i % 2) * pingpong_offset, C * sizeof(T),
                   C * sizeof(T), c_align * sizeof(T), n_seg);
    loadInput<T>(nram_input + (i % 2) * pingpong_offset,
                 (T*)input + (i + 2) * C * n_seg, C * sizeof(T),
                 c_align * sizeof(T), C * sizeof(T), n_seg);
    loadInput<int32_t>(nram_target + (i % 2) * pingpong_offset,
                       (int32_t*)target + (i + 2) * n_seg, target_size);
    compute((T*)(nram_input + ((i + 1) % 2) * pingpong_offset),
            (int32_t*)(nram_target + ((i + 1) % 2) * pingpong_offset),
            (T*)nram_weight, weight_size, deal_num, n_seg, C, alpha, gamma,
            (T*)nram_scalar, (T*)nram_compute_a, (T*)nram_compute_b,
            (T*)(nram_output + ((i + 1) % 2) * pingpong_offset));
    __asm__ volatile("sync;");
  }

  if (repeat > 1) {
    storeOutput<T>((T*)output + (repeat - 2) * C * n_seg,
                   nram_output + (repeat % 2) * pingpong_offset, C * sizeof(T),
                   C * sizeof(T), c_align * sizeof(T), n_seg);
  }
  if (remain > 0) {
    loadInput<T>(nram_input + (repeat % 2) * pingpong_offset,
                 (T*)input + repeat * C * n_seg, C * sizeof(T),
                 c_align * sizeof(T), C * sizeof(T), remain);
    loadInput<int32_t>(nram_target + (repeat % 2) * pingpong_offset,
                       (int32_t*)target + repeat * n_seg,
                       remain * sizeof(int32_t));
  }
  if (repeat > 0) {
    compute((T*)(nram_input + ((repeat - 1) % 2) * pingpong_offset),
            (int32_t*)(nram_target + ((repeat - 1) % 2) * pingpong_offset),
            (T*)nram_weight, weight_size, deal_num, n_seg, C, alpha, gamma,
            (T*)nram_scalar, (T*)nram_compute_a, (T*)nram_compute_b,
            (T*)(nram_output + ((repeat - 1) % 2) * pingpong_offset));
  }
  __asm__ volatile("sync;");

  if (repeat > 0) {
    storeOutput<T>((T*)output + (repeat - 1) * C * n_seg,
                   nram_output + ((repeat - 1) % 2) * pingpong_offset,
                   C * sizeof(T), C * sizeof(T), c_align * sizeof(T), n_seg);
  }
  if (remain > 0) {
    int rem_deal_num = remain * c_align_size / sizeof(T);
    compute((T*)(nram_input + (repeat % 2) * pingpong_offset),
            (int32_t*)(nram_target + (repeat % 2) * pingpong_offset),
            (T*)nram_weight, weight_size, rem_deal_num, remain, C, alpha, gamma,
            (T*)nram_scalar, (T*)nram_compute_a, (T*)nram_compute_b,
            (T*)(nram_output + (repeat % 2) * pingpong_offset));
    __asm__ volatile("sync;");

    storeOutput<T>((T*)output + repeat * C * n_seg,
                   nram_output + (repeat % 2) * pingpong_offset, C * sizeof(T),
                   C * sizeof(T), c_align * sizeof(T), remain);
  }
}

template <typename T>
__mlu_global__ void MLUUnion1KernelFocalLossSigmoidForward(
    const void* input, const void* target, const void* weight, const int32_t N,
    const int32_t C, const float alpha, const float gamma, void* output) {
  const int32_t n_seg = N / taskDim + (taskId == taskDim - 1) * (N % taskDim);
  const T* input_offset = (T*)input + N / taskDim * taskId * C;
  const int32_t* target_offset = (int32_t*)target + N / taskDim * taskId;
  T* output_offset = (T*)output + N / taskDim * taskId * C;

  focalLossSigmoidForwardBlock((T*)input_offset, (int32_t*)target_offset,
                               (T*)weight, n_seg, C, alpha, gamma,
                               (T*)output_offset);
}

void KernelFocalLossSigmoidForward(cnrtDim3_t k_dim, cnrtFunctionType_t k_type,
                                   cnrtQueue_t queue, cnrtDataType_t d_type,
                                   const void* input, const void* target,
                                   const void* weight, const int32_t N,
                                   const int32_t C, const float alpha,
                                   const float gamma, void* output) {
  if (d_type == CNRT_FLOAT16) {
    MLUUnion1KernelFocalLossSigmoidForward<half><<<k_dim, k_type, queue>>>(
        input, target, weight, N, C, alpha, gamma, output);
  } else {
    MLUUnion1KernelFocalLossSigmoidForward<float><<<k_dim, k_type, queue>>>(
        input, target, weight, N, C, alpha, gamma, output);
  }
}